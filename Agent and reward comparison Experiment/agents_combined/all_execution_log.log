09-02 17:50 | DEBUG   | src.AutoGrid             | 159  | Format logger configured [{'fmt': '{asctime} | {levelname:7s} | {name:24s} | {lineno:<4n} | {message}', 'style': '{', 'datefmt': '%m-%d %H:%M'}]
09-02 17:50 | DEBUG   | src.AutoGrid             | 160  | Console logger configured [{'level': 'DEBUG'}]
09-02 17:50 | DEBUG   | src.AutoGrid             | 161  | File logger configured [{'level': 'DEBUG', 'filename': './agents_combined\\all_execution_log.log', 'mode': 'a'}]
09-02 17:50 | DEBUG   | src.AutoGrid             | 90   | Configured runner <runners.base.baseRunner object at 0x0000029C02B161C0>
09-02 17:50 | INFO    | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 53   | Executing experiment file ~\AutoGrid\experiments\sb3_grid_ace\final\all_combined.py
09-02 17:50 | INFO    | src.AutoGrid             | 170  | Starting execution.
09-02 17:50 | INFO    | runners.base             | 95   | Executing experiment [experiment_Do_Nothing]
09-02 17:50 | DEBUG   | src.helpers              | 81   | Conflict at values of experiment_Do_Nothing[maker]=<function create_do_nothing_agent at 0x0000029C7BEF6160> with common[maker]=<function create_agent_sb3 at 0x0000029C02B12F70>
09-02 17:50 | DEBUG   | src.helpers              | 81   | Conflict at values of experiment_Do_Nothing[training]=None with common[training]=DEFAULT
09-02 17:50 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-02 17:50 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000029C26AF73A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000029C02B16310>}))
09-02 17:50 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x0000029C02B16400>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x0000029C02B165E0>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x0000029C02B16550>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x0000029C02B16580>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x0000029C02B16490>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x0000029C0343C5B0>, 'weight': 0.2}}
09-02 17:50 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-02 17:50 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x0000029C7BE550D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000029C02B16340>}))
09-02 17:50 | INFO    | src.trainner.trainer     | 27   | Training agent [<grid2op.Agent.doNothing.DoNothingAgent object at 0x0000029C06CC1E80>] with trainner: None({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\Do_Nothing', 'tb_log_name': 'Do_Nothing'})
09-02 17:50 | WARNING | src.trainner.trainer     | 39   | [None] is not a valid training method or class.
09-02 17:50 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-02 17:50 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x0000029C06CD5C10>
09-02 17:56 | DEBUG   | runners.base             | 82   | Execution finished, cleaning up resources.
09-02 17:56 | INFO    | runners.base             | 95   | Executing experiment [experiment_A2C_box]
09-02 17:56 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-02 17:56 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000029C26AF73A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000029C02C12D00>}))
09-02 17:56 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x0000029C037467F0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x0000029C7C7AD640>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x0000029C087C40D0>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x0000029C087C4F10>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x0000029C087C4E20>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x0000029C7BF3AFD0>, 'weight': 0.2}}
09-02 17:56 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-02 17:56 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x0000029C7BE550D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000029C7C795CD0>}))
09-02 17:56 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-02 17:56 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-02 17:56 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-02 17:56 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-02 17:56 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.box_gym_actspace.BoxGymnasiumActSpace'>  with arguments {'attr_to_keep': ['redispatch', 'curtail']}
09-02 17:56 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)]
09-02 17:56 | DEBUG   | src.makers.SB3           | 81   | tensorboard logging directory [./agents_combined\tensorboard_log] does not exist, creating it now.
09-02 17:56 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.a2c.a2c.A2C'>]
09-02 17:56 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x0000029C05839DF0>
09-02 17:56 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)
09-02 17:56 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-02 17:56 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-02 17:56 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.a2c.a2c.A2C'>
09-02 17:56 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x0000029C7C7AD9A0>, 'verbose': True}
09-02 17:56 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
09-02 17:56 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x0000029C02C26B20>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\A2C_box', 'tb_log_name': 'A2C_box'})
09-02 17:56 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
09-11 18:36 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-11 18:36 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x0000029C02C344F0>
09-11 18:40 | DEBUG   | runners.base             | 82   | Execution finished, cleaning up resources.
09-11 18:40 | INFO    | runners.base             | 95   | Executing experiment [experiment_A2C_discrete]
09-11 18:40 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-11 18:40 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000029C26AF73A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000029CB7EDCDC0>}))
09-11 18:40 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x0000029CB3993E50>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x0000029CB3993400>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x0000029CB4C0B340>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x0000029CB4C0B3D0>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x0000029CB4C0B130>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x0000029CB64A7730>, 'weight': 0.2}}
09-11 18:40 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-11 18:40 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x0000029C7BE550D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000029CB816A6A0>}))
09-11 18:40 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-11 18:40 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-11 18:40 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-11 18:40 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-11 18:40 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.discrete_gym_actspace.MultiDiscreteActSpaceGymnasium'>  with arguments {'attr_to_keep': {'curtail', 'redispatch'}}
09-11 18:40 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(205)]
09-11 18:40 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.a2c.a2c.A2C'>]
09-11 18:40 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x0000029C0370C4F0>
09-11 18:40 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(205)
09-11 18:40 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-11 18:40 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-11 18:40 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.a2c.a2c.A2C'>
09-11 18:40 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x0000029C06F0B730>, 'verbose': True}
09-11 18:40 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
09-11 18:40 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x0000029CB7DEBA30>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\A2C_discrete', 'tb_log_name': 'A2C_discrete'})
09-11 18:40 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
09-12 05:00 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-12 05:00 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x0000029C06D7C430>
09-12 05:07 | DEBUG   | runners.base             | 82   | Execution finished, cleaning up resources.
09-12 05:07 | INFO    | runners.base             | 95   | Executing experiment [experiment_A2C_discrete_singleaction]
09-12 05:07 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-12 05:07 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000029C26AF73A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000029CAD32EFA0>}))
09-12 05:07 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x0000029CB70265B0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x0000029CB62483A0>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x0000029CA46C7F10>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x0000029CA46C72E0>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x0000029CA46C78E0>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x0000029CA46C7D60>, 'weight': 0.2}}
09-12 05:07 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-12 05:07 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x0000029C7BE550D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000029CB7253BB0>}))
09-12 05:07 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-12 05:07 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-12 05:07 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-12 05:07 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-12 05:07 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <function create_discrete_action_space at 0x0000029C02B17280>  with arguments {'save_path': './discrete_action_space', 'load_path': './discrete_action_space', '_action_filter': <function _filter_action at 0x0000029C02B171F0>}
09-12 05:07 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 113  | Loaded Action space size:201 from folder [./discrete_action_space]
09-12 05:07 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(201)]
09-12 05:07 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.a2c.a2c.A2C'>]
09-12 05:07 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x0000029C88694790>
09-12 05:07 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(201)
09-12 05:07 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-12 05:07 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-12 05:07 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.a2c.a2c.A2C'>
09-12 05:07 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x0000029C08597310>, 'verbose': True}
09-12 05:07 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
09-12 05:07 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x0000029C9AA7F970>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\A2C_discrete_singleaction', 'tb_log_name': 'A2C_discrete_singleaction'})
09-12 05:07 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
09-12 19:37 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-12 19:37 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x0000029C1AB43130>
09-12 19:50 | DEBUG   | runners.base             | 82   | Execution finished, cleaning up resources.
09-12 19:50 | INFO    | runners.base             | 95   | Executing experiment [experiment_DDPG_box]
09-12 19:50 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-12 19:50 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000029C26AF73A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000029C03411C70>}))
09-12 19:50 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x0000029C94261430>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x0000029C942612E0>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x0000029C94261520>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x0000029C94261550>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x0000029C94261970>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x0000029C942616A0>, 'weight': 0.2}}
09-12 19:50 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-12 19:50 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x0000029C7BE550D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000029C942616D0>}))
09-12 19:51 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-12 19:51 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-12 19:51 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-12 19:51 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-12 19:51 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.box_gym_actspace.BoxGymnasiumActSpace'>  with arguments {'attr_to_keep': ['redispatch', 'curtail']}
09-12 19:51 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)]
09-12 19:51 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.ddpg.ddpg.DDPG'>]
09-12 19:51 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x0000029C959B3970>
09-12 19:51 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)
09-12 19:51 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-12 19:51 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-12 19:51 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.ddpg.ddpg.DDPG'>
09-12 19:51 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.td3.policies.TD3Policy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x0000029CAA392310>, 'verbose': True}
09-12 19:51 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
09-12 19:51 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x0000029CB3500580>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\DDPG_box', 'tb_log_name': 'DDPG_box'})
09-12 19:51 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
09-16 00:41 | DEBUG   | src.AutoGrid             | 158  | Format logger configured [{'fmt': '{asctime} | {levelname:7s} | {name:24s} | {lineno:<4n} | {message}', 'style': '{', 'datefmt': '%m-%d %H:%M'}]
09-16 00:41 | DEBUG   | src.AutoGrid             | 159  | Console logger configured [{'level': 'DEBUG'}]
09-16 00:41 | DEBUG   | src.AutoGrid             | 160  | File logger configured [{'level': 'DEBUG', 'filename': './agents_combined\\all_execution_log.log', 'mode': 'a'}]
09-16 00:41 | DEBUG   | src.AutoGrid             | 89   | Configured runner <runners.base.baseRunner object at 0x00000216216761C0>
09-16 00:41 | INFO    | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 53   | Executing experiment file ~\AutoGrid\experiments\sb3_grid_ace\final\all_combined.py
09-16 00:41 | INFO    | src.AutoGrid             | 169  | Starting execution.
09-16 00:41 | INFO    | runners.base             | 98   | Preparing experiment [experiment_Do_Nothing]
09-16 00:41 | DEBUG   | src.helpers              | 81   | Conflict at values of experiment_Do_Nothing[maker]=<function create_do_nothing_agent at 0x000002161ACD7160> with common[maker]=<function create_agent_sb3 at 0x0000021621672F70>
09-16 00:41 | DEBUG   | src.helpers              | 81   | Conflict at values of experiment_Do_Nothing[training]=None with common[training]=DEFAULT
09-16 00:41 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-16 00:41 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000021645E073A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000021621676310>}))
09-16 00:41 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x0000021621676400>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x00000216216765E0>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x0000021621676550>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x0000021621676580>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x0000021621676490>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x0000021622150C10>, 'weight': 0.2}}
09-16 00:41 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-16 00:41 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000002161AC360D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000021621676340>}))
09-16 00:41 | INFO    | runners.base             | 74   | Executing experiment [Do_Nothing]
09-16 00:41 | INFO    | src.trainner.trainer     | 27   | Training agent [<grid2op.Agent.doNothing.DoNothingAgent object at 0x000002162585BE80>] with trainner: None({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\Do_Nothing', 'tb_log_name': 'Do_Nothing'})
09-16 00:41 | WARNING | src.trainner.trainer     | 39   | [None] is not a valid training method or class.
09-16 00:41 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-16 00:41 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x0000021625842C10>
09-16 00:49 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
09-16 00:49 | INFO    | runners.base             | 98   | Preparing experiment [experiment_A2C_box]
09-16 00:49 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-16 00:49 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000021645E073A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000021621772D00>}))
09-16 00:49 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000002161B56A6D0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000002161B58E640>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000002162D14D7F0>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000002162D14DB50>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000002162D14D340>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000002162D14D160>, 'weight': 0.2}}
09-16 00:49 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-16 00:49 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000002161AC360D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000002161B575CD0>}))
09-16 00:49 | INFO    | runners.base             | 74   | Executing experiment [A2C_box]
09-16 00:49 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-16 00:49 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-16 00:49 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-16 00:49 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-16 00:49 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.box_gym_actspace.BoxGymnasiumActSpace'>  with arguments {'attr_to_keep': ['redispatch', 'curtail']}
09-16 00:49 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)]
09-16 00:49 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.a2c.a2c.A2C'>]
09-16 00:49 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000002162D80DD00>
09-16 00:49 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)
09-16 00:49 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-16 00:49 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-16 00:49 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.a2c.a2c.A2C'>
09-16 00:49 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x00000216245F1C10>, 'verbose': True}
09-16 00:49 | DEBUG   | src.makers.SB3           | 94   | nn_path: ./agents_combined\A2C_box.zip
09-16 00:49 | WARNING | src.agents.grid2OpGymAgent | 100  | Both nn_path and nn_kwargs are set, an agent will be loaded, not created.To create and agent please set nn_path to None
09-16 00:49 | DEBUG   | src.agents.Grid2OpSB3    | 175  | loading agent from [./agents_combined\A2C_box.zip]
09-16 00:49 | DEBUG   | src.agents.Grid2OpSB3    | 179  | Agent loaded with  10000000 total_timesteps trained
09-16 00:49 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x00000216244733D0>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\A2C_box', 'tb_log_name': 'A2C_box'})
09-16 00:49 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 0 timesteps
09-16 00:49 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-16 00:49 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x0000021625A32FA0>
09-16 00:54 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
09-16 00:54 | INFO    | runners.base             | 98   | Preparing experiment [experiment_A2C_discrete]
09-16 00:54 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-16 00:54 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000021645E073A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000021621768040>}))
09-16 00:54 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x0000021624635370>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x0000021624635280>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x0000021624635640>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x0000021624635700>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x0000021624635310>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x0000021624635610>, 'weight': 0.2}}
09-16 00:54 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-16 00:54 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000002161AC360D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000021624635760>}))
09-16 00:54 | INFO    | runners.base             | 74   | Executing experiment [A2C_discrete]
09-16 00:54 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-16 00:54 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-16 00:54 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-16 00:54 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-16 00:54 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.discrete_gym_actspace.MultiDiscreteActSpaceGymnasium'>  with arguments {'attr_to_keep': {'redispatch', 'curtail'}}
09-16 00:54 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(205)]
09-16 00:54 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.a2c.a2c.A2C'>]
09-16 00:54 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000002162DCB2100>
09-16 00:54 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(205)
09-16 00:54 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-16 00:54 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-16 00:54 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.a2c.a2c.A2C'>
09-16 00:54 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x0000021624491F10>, 'verbose': True}
09-16 00:54 | DEBUG   | src.makers.SB3           | 94   | nn_path: ./agents_combined\A2C_discrete.zip
09-16 00:54 | WARNING | src.agents.grid2OpGymAgent | 100  | Both nn_path and nn_kwargs are set, an agent will be loaded, not created.To create and agent please set nn_path to None
09-16 00:54 | DEBUG   | src.agents.Grid2OpSB3    | 175  | loading agent from [./agents_combined\A2C_discrete.zip]
09-16 00:54 | DEBUG   | src.agents.Grid2OpSB3    | 179  | Agent loaded with  10000000 total_timesteps trained
09-16 00:54 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x0000021629FE0220>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\A2C_discrete', 'tb_log_name': 'A2C_discrete'})
09-16 00:54 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 0 timesteps
09-16 00:54 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-16 00:54 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x0000021629A8A4F0>
09-16 01:06 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
09-16 01:06 | INFO    | runners.base             | 98   | Preparing experiment [experiment_A2C_discrete_singleaction]
09-16 01:06 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-16 01:06 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000021645E073A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000002162D773430>}))
09-16 01:06 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000002162D773EB0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000002162D773760>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000002162D773C10>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000002162D773A30>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000002162D773640>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000002162D773E20>, 'weight': 0.2}}
09-16 01:06 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-16 01:06 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000002161AC360D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000002162D7730A0>}))
09-16 01:06 | INFO    | runners.base             | 74   | Executing experiment [A2C_discrete_singleaction]
09-16 01:06 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-16 01:06 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-16 01:06 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-16 01:06 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-16 01:06 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <function create_discrete_action_space at 0x0000021621678280>  with arguments {'save_path': './discrete_action_space', 'load_path': './discrete_action_space', '_action_filter': <function _filter_action at 0x00000216216781F0>}
09-16 01:06 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 113  | Loaded Action space size:201 from folder [./discrete_action_space]
09-16 01:06 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(201)]
09-16 01:06 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.a2c.a2c.A2C'>]
09-16 01:06 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x0000021644E993A0>
09-16 01:06 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(201)
09-16 01:06 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-16 01:06 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-16 01:06 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.a2c.a2c.A2C'>
09-16 01:06 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x0000021627B27AF0>, 'verbose': True}
09-16 01:06 | DEBUG   | src.makers.SB3           | 94   | nn_path: ./agents_combined\A2C_discrete_singleaction.zip
09-16 01:06 | WARNING | src.agents.grid2OpGymAgent | 100  | Both nn_path and nn_kwargs are set, an agent will be loaded, not created.To create and agent please set nn_path to None
09-16 01:06 | DEBUG   | src.agents.Grid2OpSB3    | 175  | loading agent from [./agents_combined\A2C_discrete_singleaction.zip]
09-16 01:06 | DEBUG   | src.agents.Grid2OpSB3    | 179  | Agent loaded with  10000000 total_timesteps trained
09-16 01:06 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x00000216321E0DC0>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\A2C_discrete_singleaction', 'tb_log_name': 'A2C_discrete_singleaction'})
09-16 01:06 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 0 timesteps
09-16 01:06 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-16 01:06 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000002162A9FC0A0>
09-16 01:17 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
09-16 01:17 | INFO    | runners.base             | 98   | Preparing experiment [experiment_DDPG_box]
09-16 01:17 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-16 01:17 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000021645E073A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x00000216272503A0>}))
09-16 01:17 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x00000216382B6520>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x00000216382B6C40>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x00000216382B6C70>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x00000216382B6940>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x00000216382B6D00>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x00000216382B6460>, 'weight': 0.2}}
09-16 01:17 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-16 01:17 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000002161AC360D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x00000216382B6FD0>}))
09-16 01:17 | INFO    | runners.base             | 74   | Executing experiment [DDPG_box]
09-16 01:17 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-16 01:17 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-16 01:17 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-16 01:17 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-16 01:17 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.box_gym_actspace.BoxGymnasiumActSpace'>  with arguments {'attr_to_keep': ['redispatch', 'curtail']}
09-16 01:17 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)]
09-16 01:17 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.ddpg.ddpg.DDPG'>]
09-16 01:17 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x00000216850F9B20>
09-16 01:17 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)
09-16 01:17 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-16 01:17 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-16 01:17 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.ddpg.ddpg.DDPG'>
09-16 01:17 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.td3.policies.TD3Policy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x0000021627ABA670>, 'verbose': True}
09-16 01:17 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
09-16 01:17 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x0000021699C5CB50>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\DDPG_box', 'tb_log_name': 'DDPG_box'})
09-16 01:17 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
09-21 22:19 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-21 22:19 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000002164102BD90>
09-21 22:23 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
09-21 22:23 | INFO    | runners.base             | 98   | Preparing experiment [experiment_DQN_discrete]
09-21 22:23 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-21 22:23 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000021645E073A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000002164479F310>}))
09-21 22:23 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x0000021640531B80>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x0000021640E63FD0>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x00000216447BE400>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x00000216447BE640>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x00000216447BEB80>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x00000216447BE7F0>, 'weight': 0.2}}
09-21 22:23 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-21 22:23 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000002161AC360D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000002163683C790>}))
09-21 22:23 | INFO    | runners.base             | 74   | Executing experiment [DQN_discrete]
09-21 22:23 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-21 22:23 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-21 22:23 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-21 22:23 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-21 22:23 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.discrete_gym_actspace.MultiDiscreteActSpaceGymnasium'>  with arguments {'attr_to_keep': {'redispatch', 'curtail'}}
09-21 22:23 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(205)]
09-21 22:23 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.dqn.dqn.DQN'>]
09-21 22:23 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000002162A4D00A0>
09-21 22:23 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(205)
09-21 22:23 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-21 22:23 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-21 22:23 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.dqn.dqn.DQN'>
09-21 22:23 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.dqn.policies.DQNPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x0000021644C309D0>, 'verbose': True}
09-21 22:23 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
09-21 22:23 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x0000021627187AC0>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\DQN_discrete', 'tb_log_name': 'DQN_discrete'})
09-21 22:23 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
09-22 12:33 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-22 12:33 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000002168F55D4F0>
09-22 12:37 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
09-22 12:37 | INFO    | runners.base             | 98   | Preparing experiment [experiment_DQN_discrete_singleaction]
09-22 12:37 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-22 12:37 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000021645E073A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000021625FA7250>}))
09-22 12:37 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000002169AE17F10>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000002164006ECD0>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x0000021637C70430>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x0000021637C70C40>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x0000021637C70F40>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x00000216294EF730>, 'weight': 0.2}}
09-22 12:37 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-22 12:37 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000002161AC360D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000002162A6F05E0>}))
09-22 12:37 | INFO    | runners.base             | 74   | Executing experiment [DQN_discrete_singleaction]
09-22 12:37 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-22 12:37 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-22 12:37 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-22 12:37 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-22 12:37 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <function create_discrete_action_space at 0x0000021621678280>  with arguments {'save_path': './discrete_action_space', 'load_path': './discrete_action_space', '_action_filter': <function _filter_action at 0x00000216216781F0>}
09-22 12:37 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 113  | Loaded Action space size:201 from folder [./discrete_action_space]
09-22 12:37 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(201)]
09-22 12:37 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.dqn.dqn.DQN'>]
09-22 12:37 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x00000216849537F0>
09-22 12:37 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(201)
09-22 12:37 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-22 12:37 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-22 12:37 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.dqn.dqn.DQN'>
09-22 12:37 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.dqn.policies.DQNPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x0000021629ADC6D0>, 'verbose': True}
09-22 12:37 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
09-22 12:37 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x0000021640D26A60>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\DQN_discrete_singleaction', 'tb_log_name': 'DQN_discrete_singleaction'})
09-22 12:37 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
09-23 15:15 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
09-23 15:15 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x0000021644ADC6D0>
09-23 15:23 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
09-23 15:23 | INFO    | runners.base             | 98   | Preparing experiment [experiment_PPO_box]
09-23 15:23 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
09-23 15:23 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x0000021645E073A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x0000021632C20280>}))
09-23 15:23 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000002162DCF9AC0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x0000021636788C40>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x0000021644C9ED60>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x0000021682A984C0>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x0000021682FED4F0>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x0000021636795D90>, 'weight': 0.2}}
09-23 15:23 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
09-23 15:23 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000002161AC360D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x00000216367D5FD0>}))
09-23 15:23 | INFO    | runners.base             | 74   | Executing experiment [PPO_box]
09-23 15:23 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
09-23 15:23 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
09-23 15:23 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
09-23 15:23 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
09-23 15:23 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.box_gym_actspace.BoxGymnasiumActSpace'>  with arguments {'attr_to_keep': ['redispatch', 'curtail']}
09-23 15:23 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)]
09-23 15:23 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.ppo.ppo.PPO'>]
09-23 15:23 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x0000021629A6CEB0>
09-23 15:23 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)
09-23 15:23 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
09-23 15:23 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
09-23 15:23 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.ppo.ppo.PPO'>
09-23 15:23 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x0000021640FDDD00>, 'verbose': True}
09-23 15:23 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
09-23 15:23 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x00000216270C3C10>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\PPO_box', 'tb_log_name': 'PPO_box'})
09-23 15:23 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
10-09 10:19 | DEBUG   | src.AutoGrid             | 158  | Format logger configured [{'fmt': '{asctime} | {levelname:7s} | {name:24s} | {lineno:<4n} | {message}', 'style': '{', 'datefmt': '%m-%d %H:%M'}]
10-09 10:19 | DEBUG   | src.AutoGrid             | 159  | Console logger configured [{'level': 'DEBUG'}]
10-09 10:19 | DEBUG   | src.AutoGrid             | 160  | File logger configured [{'level': 'DEBUG', 'filename': './agents_combined\\all_execution_log.log', 'mode': 'a'}]
10-09 10:19 | DEBUG   | src.AutoGrid             | 89   | Configured runner <runners.base.baseRunner object at 0x000001AAD55561C0>
10-09 10:19 | INFO    | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 53   | Executing experiment file ~\AutoGrid\experiments\sb3_grid_ace\final\all_combined.py
10-09 10:19 | INFO    | src.AutoGrid             | 169  | Starting execution.
10-09 10:19 | INFO    | runners.base             | 98   | Preparing experiment [experiment_Do_Nothing]
10-09 10:19 | DEBUG   | src.helpers              | 81   | Conflict at values of experiment_Do_Nothing[maker]=<function create_do_nothing_agent at 0x000001AACEBB6160> with common[maker]=<function create_agent_sb3 at 0x000001AAD5552F70>
10-09 10:19 | DEBUG   | src.helpers              | 81   | Conflict at values of experiment_Do_Nothing[training]=None with common[training]=DEFAULT
10-09 10:19 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
10-09 10:19 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x000001AAF9E273A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AAD5556310>}))
10-09 10:19 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000001AAD5556400>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000001AAD55565E0>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000001AAD5556550>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000001AAD5556580>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000001AAD5556490>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000001AAD6030610>, 'weight': 0.2}}
10-09 10:19 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
10-09 10:19 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000001AACEB150D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AAD5556340>}))
10-09 10:19 | INFO    | runners.base             | 74   | Executing experiment [Do_Nothing]
10-09 10:19 | INFO    | src.trainner.trainer     | 27   | Training agent [<grid2op.Agent.doNothing.DoNothingAgent object at 0x000001AAD884DA60>] with trainner: None({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\Do_Nothing', 'tb_log_name': 'Do_Nothing'})
10-09 10:19 | WARNING | src.trainner.trainer     | 39   | [None] is not a valid training method or class.
10-09 10:19 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
10-09 10:19 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000001AAD8816C40>
10-09 10:27 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
10-09 10:27 | INFO    | runners.base             | 98   | Preparing experiment [experiment_A2C_box]
10-09 10:27 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
10-09 10:27 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x000001AAF9E273A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AAD5652D00>}))
10-09 10:27 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000001AACF44A6D0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000001AACF46E640>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000001AAD81FF310>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000001AAD81FF0D0>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000001AAD81FF5E0>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000001AAD81FF2B0>, 'weight': 0.2}}
10-09 10:27 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
10-09 10:27 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000001AACEB150D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AACF455CD0>}))
10-09 10:27 | INFO    | runners.base             | 74   | Executing experiment [A2C_box]
10-09 10:27 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
10-09 10:27 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
10-09 10:27 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
10-09 10:27 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
10-09 10:27 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.box_gym_actspace.BoxGymnasiumActSpace'>  with arguments {'attr_to_keep': ['redispatch', 'curtail']}
10-09 10:27 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)]
10-09 10:27 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.a2c.a2c.A2C'>]
10-09 10:27 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000001AADD2475E0>
10-09 10:27 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)
10-09 10:27 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
10-09 10:27 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
10-09 10:27 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.a2c.a2c.A2C'>
10-09 10:27 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x000001AADD1F70D0>, 'verbose': True}
10-09 10:27 | DEBUG   | src.makers.SB3           | 94   | nn_path: ./agents_combined\A2C_box.zip
10-09 10:27 | WARNING | src.agents.grid2OpGymAgent | 100  | Both nn_path and nn_kwargs are set, an agent will be loaded, not created.To create and agent please set nn_path to None
10-09 10:27 | DEBUG   | src.agents.Grid2OpSB3    | 175  | loading agent from [./agents_combined\A2C_box.zip]
10-09 10:27 | DEBUG   | src.agents.Grid2OpSB3    | 179  | Agent loaded with  10000000 total_timesteps trained
10-09 10:27 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x000001AADAD72760>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\A2C_box', 'tb_log_name': 'A2C_box'})
10-09 10:27 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 0 timesteps
10-09 10:27 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
10-09 10:27 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000001AADB44EEB0>
10-09 10:32 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
10-09 10:32 | INFO    | runners.base             | 98   | Preparing experiment [experiment_A2C_discrete]
10-09 10:32 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
10-09 10:32 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x000001AAF9E273A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AAD9A0D490>}))
10-09 10:32 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000001AAD9A0D6D0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000001AAD9A0D910>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000001AAD9A0DA00>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000001AAD9A0D610>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000001AAD9A0DC10>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000001AAD9A0D7C0>, 'weight': 0.2}}
10-09 10:32 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
10-09 10:32 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000001AACEB150D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AAD9A0D670>}))
10-09 10:32 | INFO    | runners.base             | 74   | Executing experiment [A2C_discrete]
10-09 10:32 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
10-09 10:32 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
10-09 10:32 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
10-09 10:32 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
10-09 10:32 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.discrete_gym_actspace.MultiDiscreteActSpaceGymnasium'>  with arguments {'attr_to_keep': {'redispatch', 'curtail'}}
10-09 10:32 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(205)]
10-09 10:32 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.a2c.a2c.A2C'>]
10-09 10:32 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000001AAE68F0A00>
10-09 10:32 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(205)
10-09 10:32 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
10-09 10:32 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
10-09 10:32 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.a2c.a2c.A2C'>
10-09 10:32 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x000001AADAD72640>, 'verbose': True}
10-09 10:32 | DEBUG   | src.makers.SB3           | 94   | nn_path: ./agents_combined\A2C_discrete.zip
10-09 10:32 | WARNING | src.agents.grid2OpGymAgent | 100  | Both nn_path and nn_kwargs are set, an agent will be loaded, not created.To create and agent please set nn_path to None
10-09 10:32 | DEBUG   | src.agents.Grid2OpSB3    | 175  | loading agent from [./agents_combined\A2C_discrete.zip]
10-09 10:32 | DEBUG   | src.agents.Grid2OpSB3    | 179  | Agent loaded with  10000000 total_timesteps trained
10-09 10:32 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x000001AADAC34790>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\A2C_discrete', 'tb_log_name': 'A2C_discrete'})
10-09 10:32 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 0 timesteps
10-09 10:32 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
10-09 10:32 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000001AAD82AAC10>
10-09 10:44 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
10-09 10:44 | INFO    | runners.base             | 98   | Preparing experiment [experiment_A2C_discrete_singleaction]
10-09 10:44 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
10-09 10:44 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x000001AAF9E273A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AACF44A6D0>}))
10-09 10:44 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000001AB1C6ABA00>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000001AB1C6AB0A0>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000001AB1C6AB820>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000001AB1C6AB130>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000001AB1C6ABAF0>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000001AB1C6ABA60>, 'weight': 0.2}}
10-09 10:44 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
10-09 10:44 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000001AACEB150D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AB1C6ABBB0>}))
10-09 10:44 | INFO    | runners.base             | 74   | Executing experiment [A2C_discrete_singleaction]
10-09 10:44 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
10-09 10:44 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
10-09 10:44 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
10-09 10:44 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
10-09 10:44 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <function create_discrete_action_space at 0x000001AAD5558280>  with arguments {'save_path': './discrete_action_space', 'load_path': './discrete_action_space', '_action_filter': <function _filter_action at 0x000001AAD55581F0>}
10-09 10:44 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 113  | Loaded Action space size:201 from folder [./discrete_action_space]
10-09 10:44 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(201)]
10-09 10:44 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.a2c.a2c.A2C'>]
10-09 10:44 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000001AAF0EB6220>
10-09 10:44 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(201)
10-09 10:44 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
10-09 10:44 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
10-09 10:44 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.a2c.a2c.A2C'>
10-09 10:44 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x000001AB1C6ABE80>, 'verbose': True}
10-09 10:44 | DEBUG   | src.makers.SB3           | 94   | nn_path: ./agents_combined\A2C_discrete_singleaction.zip
10-09 10:44 | WARNING | src.agents.grid2OpGymAgent | 100  | Both nn_path and nn_kwargs are set, an agent will be loaded, not created.To create and agent please set nn_path to None
10-09 10:44 | DEBUG   | src.agents.Grid2OpSB3    | 175  | loading agent from [./agents_combined\A2C_discrete_singleaction.zip]
10-09 10:44 | DEBUG   | src.agents.Grid2OpSB3    | 179  | Agent loaded with  10000000 total_timesteps trained
10-09 10:44 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x000001AAE928C9D0>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\A2C_discrete_singleaction', 'tb_log_name': 'A2C_discrete_singleaction'})
10-09 10:44 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 0 timesteps
10-09 10:44 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
10-09 10:44 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000001AAEB5B6CD0>
10-09 10:55 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
10-09 10:55 | INFO    | runners.base             | 98   | Preparing experiment [experiment_DDPG_box]
10-09 10:55 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
10-09 10:55 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x000001AAF9E273A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AADA1CFB80>}))
10-09 10:55 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000001AADCB95E20>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000001AADCB95B50>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000001AADCB95C10>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000001AADCB95F70>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000001AADCB95220>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000001AADCB95820>, 'weight': 0.2}}
10-09 10:55 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
10-09 10:55 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000001AACEB150D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AB1C82B8B0>}))
10-09 10:55 | INFO    | runners.base             | 74   | Executing experiment [DDPG_box]
10-09 10:55 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
10-09 10:55 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
10-09 10:55 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
10-09 10:55 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
10-09 10:55 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.box_gym_actspace.BoxGymnasiumActSpace'>  with arguments {'attr_to_keep': ['redispatch', 'curtail']}
10-09 10:55 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)]
10-09 10:55 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.ddpg.ddpg.DDPG'>]
10-09 10:55 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000001AAEB41F730>
10-09 10:55 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)
10-09 10:55 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
10-09 10:55 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
10-09 10:55 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.ddpg.ddpg.DDPG'>
10-09 10:55 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.td3.policies.TD3Policy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x000001AADCB95C70>, 'verbose': True}
10-09 10:55 | DEBUG   | src.makers.SB3           | 94   | nn_path: ./agents_combined\DDPG_box.zip
10-09 10:55 | WARNING | src.agents.grid2OpGymAgent | 100  | Both nn_path and nn_kwargs are set, an agent will be loaded, not created.To create and agent please set nn_path to None
10-09 10:55 | DEBUG   | src.agents.Grid2OpSB3    | 175  | loading agent from [./agents_combined\DDPG_box.zip]
10-09 10:55 | DEBUG   | src.agents.Grid2OpSB3    | 179  | Agent loaded with  10000000 total_timesteps trained
10-09 10:55 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x000001AADCD82B20>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\DDPG_box', 'tb_log_name': 'DDPG_box'})
10-09 10:55 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 0 timesteps
10-09 10:55 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
10-09 10:55 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000001AB0975D1C0>
10-09 11:00 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
10-09 11:00 | INFO    | runners.base             | 98   | Preparing experiment [experiment_DQN_discrete]
10-09 11:00 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
10-09 11:00 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x000001AAF9E273A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AAEDBADA30>}))
10-09 11:00 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000001AAEDBAD4C0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000001AAEDBADB20>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000001AAEDBADBB0>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000001AAEDBAD880>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000001AAEDBADA00>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000001AAEDBAD340>, 'weight': 0.2}}
10-09 11:00 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
10-09 11:00 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000001AACEB150D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AAEDBAD2E0>}))
10-09 11:00 | INFO    | runners.base             | 74   | Executing experiment [DQN_discrete]
10-09 11:00 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
10-09 11:00 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
10-09 11:00 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
10-09 11:00 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
10-09 11:00 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.discrete_gym_actspace.MultiDiscreteActSpaceGymnasium'>  with arguments {'attr_to_keep': {'redispatch', 'curtail'}}
10-09 11:00 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(205)]
10-09 11:00 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.dqn.dqn.DQN'>]
10-09 11:00 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000001AAD5FE3A90>
10-09 11:00 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(205)
10-09 11:00 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
10-09 11:00 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
10-09 11:00 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.dqn.dqn.DQN'>
10-09 11:00 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.dqn.policies.DQNPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x000001AAF0E0B340>, 'verbose': True}
10-09 11:00 | DEBUG   | src.makers.SB3           | 94   | nn_path: ./agents_combined\DQN_discrete.zip
10-09 11:00 | WARNING | src.agents.grid2OpGymAgent | 100  | Both nn_path and nn_kwargs are set, an agent will be loaded, not created.To create and agent please set nn_path to None
10-09 11:00 | DEBUG   | src.agents.Grid2OpSB3    | 175  | loading agent from [./agents_combined\DQN_discrete.zip]
10-09 11:00 | DEBUG   | src.agents.Grid2OpSB3    | 179  | Agent loaded with  10000000 total_timesteps trained
10-09 11:00 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x000001AADAC5E400>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\DQN_discrete', 'tb_log_name': 'DQN_discrete'})
10-09 11:00 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 0 timesteps
10-09 11:00 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
10-09 11:00 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000001AAED4097C0>
10-09 11:07 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
10-09 11:07 | INFO    | runners.base             | 98   | Preparing experiment [experiment_DQN_discrete_singleaction]
10-09 11:07 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
10-09 11:07 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x000001AAF9E273A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AADDDB3EB0>}))
10-09 11:07 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000001AAF0C0C8E0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000001AAF0C0CE20>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000001AAF0C0C430>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000001AAF0C0C8B0>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000001AAF0C0C3D0>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000001AAF0C0CEE0>, 'weight': 0.2}}
10-09 11:07 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
10-09 11:07 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000001AACEB150D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AAF0C0C970>}))
10-09 11:07 | INFO    | runners.base             | 74   | Executing experiment [DQN_discrete_singleaction]
10-09 11:07 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
10-09 11:07 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
10-09 11:07 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
10-09 11:07 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
10-09 11:07 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <function create_discrete_action_space at 0x000001AAD5558280>  with arguments {'save_path': './discrete_action_space', 'load_path': './discrete_action_space', '_action_filter': <function _filter_action at 0x000001AAD55581F0>}
10-09 11:07 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 113  | Loaded Action space size:201 from folder [./discrete_action_space]
10-09 11:07 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(201)]
10-09 11:07 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.dqn.dqn.DQN'>]
10-09 11:07 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000001AAF15E1BE0>
10-09 11:07 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(201)
10-09 11:07 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
10-09 11:07 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
10-09 11:07 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.dqn.dqn.DQN'>
10-09 11:07 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.dqn.policies.DQNPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x000001AAE8104DF0>, 'verbose': True}
10-09 11:07 | DEBUG   | src.makers.SB3           | 94   | nn_path: ./agents_combined\DQN_discrete_singleaction.zip
10-09 11:07 | WARNING | src.agents.grid2OpGymAgent | 100  | Both nn_path and nn_kwargs are set, an agent will be loaded, not created.To create and agent please set nn_path to None
10-09 11:07 | DEBUG   | src.agents.Grid2OpSB3    | 175  | loading agent from [./agents_combined\DQN_discrete_singleaction.zip]
10-09 11:07 | DEBUG   | src.agents.Grid2OpSB3    | 179  | Agent loaded with  10000000 total_timesteps trained
10-09 11:07 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x000001AAEDBFBA00>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\DQN_discrete_singleaction', 'tb_log_name': 'DQN_discrete_singleaction'})
10-09 11:07 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 0 timesteps
10-09 11:07 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
10-09 11:07 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000001AAEF355280>
10-09 11:16 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
10-09 11:16 | INFO    | runners.base             | 98   | Preparing experiment [experiment_PPO_box]
10-09 11:16 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
10-09 11:16 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x000001AAF9E273A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AB0B9A8760>}))
10-09 11:16 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000001AB0B9A83A0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000001AB0B9A8BB0>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000001AB0B9A8A00>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000001AB0B9A8DC0>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000001AB0B9A88B0>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000001AB0B9A8610>, 'weight': 0.2}}
10-09 11:16 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
10-09 11:16 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000001AACEB150D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AB0B9A8EB0>}))
10-09 11:16 | INFO    | runners.base             | 74   | Executing experiment [PPO_box]
10-09 11:16 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
10-09 11:16 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
10-09 11:16 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
10-09 11:16 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
10-09 11:16 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.box_gym_actspace.BoxGymnasiumActSpace'>  with arguments {'attr_to_keep': ['redispatch', 'curtail']}
10-09 11:16 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)]
10-09 11:16 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.ppo.ppo.PPO'>]
10-09 11:16 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000001AADA9C6310>
10-09 11:16 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Box([  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  -1.4  -1.4 -10.4  -1.4  -2.8  -2.8  -4.3  -2.8  -8.5  -9.9], [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.4  1.4
 10.4  1.4  2.8  2.8  4.3  2.8  8.5  9.9], (22,), float32)
10-09 11:16 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
10-09 11:16 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
10-09 11:16 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.ppo.ppo.PPO'>
10-09 11:16 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x000001AB0B9A8190>, 'verbose': True}
10-09 11:16 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
10-09 11:16 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x000001AB0AB83520>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\PPO_box', 'tb_log_name': 'PPO_box'})
10-09 11:16 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
10-29 06:07 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
10-29 06:07 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000001AB097490D0>
10-29 06:12 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
10-29 06:12 | INFO    | runners.base             | 98   | Preparing experiment [experiment_PPO_discrete]
10-29 06:12 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
10-29 06:12 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x000001AAF9E273A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AB6B2504C0>}))
10-29 06:12 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000001AB76B29F70>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000001AB77CBF460>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000001AB7185C190>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000001AB77550D90>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000001AB777C3610>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000001AB77BB4D30>, 'weight': 0.2}}
10-29 06:12 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
10-29 06:12 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000001AACEB150D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AB77481A30>}))
10-29 06:12 | INFO    | runners.base             | 74   | Executing experiment [PPO_discrete]
10-29 06:12 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
10-29 06:12 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
10-29 06:12 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
10-29 06:12 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
10-29 06:12 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <class 'grid2op.gym_compat.discrete_gym_actspace.MultiDiscreteActSpaceGymnasium'>  with arguments {'attr_to_keep': {'redispatch', 'curtail'}}
10-29 06:12 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(205)]
10-29 06:12 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.ppo.ppo.PPO'>]
10-29 06:12 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000001AAF0A36FA0>
10-29 06:12 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(205)
10-29 06:12 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
10-29 06:12 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
10-29 06:12 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.ppo.ppo.PPO'>
10-29 06:12 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x000001AAEFA21520>, 'verbose': True}
10-29 06:12 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
10-29 06:12 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x000001AB7786B910>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\PPO_discrete', 'tb_log_name': 'PPO_discrete'})
10-29 06:12 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
10-30 14:36 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
10-30 14:36 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000001AAECE94460>
10-30 14:41 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
10-30 14:41 | INFO    | runners.base             | 98   | Preparing experiment [experiment_PPO_singleaction]
10-30 14:41 | DEBUG   | runners.base             | 37   | Creating backend [<class 'lightsim2grid.lightSimBackend.LightSimBackend'>]
10-30 14:41 | DEBUG   | runners.base             | 50   | Creating a new environment using <function create_combined_reward_env at 0x000001AAF9E273A0>(([], {'dataset': 'l2rpn_icaps_2021_large_train', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.combinedScaledReward.CombinedScaledReward'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AB35A52340>}))
10-30 14:41 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 29   | Configured rewards: {'Bridge': {'instance': <grid2op.Reward.bridgeReward.BridgeReward object at 0x000001AB62F670A0>, 'weight': 0.2}, 'CloseToOverflow': {'instance': <grid2op.Reward.closeToOverflowReward.CloseToOverflowReward object at 0x000001AB62F67EE0>, 'weight': 0.2}, 'Distance': {'instance': <grid2op.Reward.distanceReward.DistanceReward object at 0x000001AB62F670D0>, 'weight': 0.2}, 'Economic': {'instance': <grid2op.Reward.economicReward.EconomicReward object at 0x000001AB62F67070>, 'weight': 0.2}, 'EpisodeDuration': {'instance': <grid2op.Reward.episodeDurationReward.EpisodeDurationReward object at 0x000001AB62F679A0>, 'weight': 0.2}, 'RedispReward': {'instance': <grid2op.Reward.redispReward.RedispReward object at 0x000001AB62F679D0>, 'weight': 0.2}}
10-30 14:41 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 30   | Reward Range: [-2.0, 41.3912467956543], [-0.5, 0.5]
10-30 14:41 | DEBUG   | runners.base             | 61   | Creating a new evaluation environment using <function make at 0x000001AACEB150D0>(([], {'dataset': 'l2rpn_icaps_2021_large_val', 'difficulty': 'competition', 'reward_class': <class 'grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore'>, 'backend': <lightsim2grid.lightSimBackend.LightSimBackend object at 0x000001AB767B9820>}))
10-30 14:41 | INFO    | runners.base             | 74   | Executing experiment [PPO_discrete_singleaction]
10-30 14:41 | DEBUG   | src.makers.SB3           | 24   | Environment [<Environment_l2rpn_icaps_2021_large_train instance named l2rpn_icaps_2021_large_train>]
10-30 14:41 | DEBUG   | src.makers.SB3           | 40   | Gym Environment [<CustomGymEnv instance>]
10-30 14:41 | DEBUG   | src.makers.SB3           | 50   | Creating new gym observation space using <class 'grid2op.gym_compat.box_gym_obsspace.BoxGymnasiumObsSpace'> with arguments {'attr_to_keep': ['gen_p', 'gen_q', 'gen_v', 'gen_margin_up', 'gen_margin_down', 'load_p', 'load_q', 'load_v', 'p_or', 'q_or', 'v_or', 'a_or', 'p_ex', 'q_ex', 'v_ex', 'a_ex', 'rho', 'line_status', 'timestep_overflow', 'target_dispatch', 'actual_dispatch', 'curtailment', 'curtailment_limit', 'thermal_limit', 'theta_or', 'theta_ex', 'load_theta', 'gen_theta']}
10-30 14:41 | DEBUG   | src.makers.SB3           | 57   | Gym observation_space [Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)]
10-30 14:41 | DEBUG   | src.makers.SB3           | 67   | Creating new gym action space using <function create_discrete_action_space at 0x000001AAD5558280>  with arguments {'save_path': './discrete_action_space', 'load_path': './discrete_action_space', '_action_filter': <function _filter_action at 0x000001AAD55581F0>}
10-30 14:41 | DEBUG   | experiments.sb3_grid_ace.final.base_IncreasingFlatReward | 113  | Loaded Action space size:201 from folder [./discrete_action_space]
10-30 14:41 | DEBUG   | src.makers.SB3           | 74   | Gym action_space [Discrete(201)]
10-30 14:41 | DEBUG   | src.makers.SB3           | 87   | Creating agent [<class 'stable_baselines3.ppo.ppo.PPO'>]
10-30 14:41 | DEBUG   | src.makers.SB3           | 88   | env.action_space: <grid2op.Space.GridObjects.ActionSpace_l2rpn_icaps_2021_large_train object at 0x000001AB2472F850>
10-30 14:41 | DEBUG   | src.makers.SB3           | 89   | env_gym.action_space: Discrete(201)
10-30 14:41 | DEBUG   | src.makers.SB3           | 90   | env_gym.observation_space: Box([0. 0. 0. ... 0. 0. 0.], [inf inf inf ... inf inf inf], (1194,), float32)
10-30 14:41 | DEBUG   | src.makers.SB3           | 91   | gymenv: <CustomGymEnv instance>
10-30 14:41 | DEBUG   | src.makers.SB3           | 92   | nn_type: <class 'stable_baselines3.ppo.ppo.PPO'>
10-30 14:41 | DEBUG   | src.makers.SB3           | 93   | nn_kwargs: {'policy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'tensorboard_log': './agents_combined\\tensorboard_log', 'env': <experiments.sb3_grid_ace.final.base_IncreasingFlatReward.CustomGymEnv object at 0x000001AB7790D790>, 'verbose': True}
10-30 14:41 | DEBUG   | src.makers.SB3           | 94   | nn_path: None
10-30 14:41 | INFO    | src.trainner.trainer     | 27   | Training agent [<src.agents.Grid2OpSB3.SB3AgentGrid2Op object at 0x000001AB474FB550>] with trainner: DEFAULT({'total_timesteps': 10000000, 'reset_num_timesteps': False, 'add_training': False, 'save_path': './agents_combined\\PPO_discrete_singleaction', 'tb_log_name': 'PPO_discrete_singleaction'})
10-30 14:41 | DEBUG   | src.agents.Grid2OpSB3    | 233  | Training agent for 10000000 timesteps
11-02 15:22 | DEBUG   | src.evaluators.evaluator | 31   | evaluation env: <Environment_l2rpn_icaps_2021_large_val instance named l2rpn_icaps_2021_large_val>
11-02 15:22 | DEBUG   | src.evaluators.evaluator | 33   | evaluation reward: <grid2op.Reward.l2RPNSandBoxScore.L2RPNSandBoxScore object at 0x000001AB2CB34CD0>
11-02 15:25 | DEBUG   | runners.base             | 85   | Execution finished, cleaning up resources.
11-02 15:25 | INFO    | src.AutoGrid             | 171  | Finished execution.
